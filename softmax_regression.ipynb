{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "Softmax regression (also called multinomial logistic regression) is a generalization of logistic regression that we can use for multi-class classification problems. In problems where you want to classify an instance into one of multiple possible classes, softmax regression is a commonly used method.\n",
    "\n",
    "Here's a step-by-step explanation:\n",
    "\n",
    "1. **Linear Function**: This is the first step in softmax regression. For each class, a separate linear function is learned. If you have `n` features and `m` classes, you will learn `m*(n+1)` parameters (weights and biases). The linear function is given by `y = Wx + b`, where `W` is the weight matrix, `x` is the input feature vector, and `b` is the bias vector. Each class has its own `W` and `b`.\n",
    "\n",
    "2. **Softmax Function**: The softmax function is used to convert the output of the linear functions into probabilities. The softmax function's output vector will have the same dimension as the input vector, with each value between 0 and 1, and the total sum of the values will be 1. The softmax function `S` for a vector `Z` is defined as `S(Z)_j = e^(Z_j) / Σ e^(Z_k)` for `j = 1, ..., K` and `k = 1, ..., K`.\n",
    "\n",
    "3. **Probability Distribution**: The output of the softmax function is a probability distribution over `K` different possible outcomes. Each element of the output vector represents the probability that the input vector belongs to a specific class.\n",
    "\n",
    "4. **Prediction**: The prediction is simply the class with the highest estimated probability. This is computed with `argmax(i)`, where `i` ranges over the classes.\n",
    "\n",
    "5. **Cross-Entropy Loss**: The cross-entropy loss function is used to measure the dissimilarity between the predicted and true distributions. It's defined as `L = - Σ y_i * log(y'_i)`, where `y` is the true distribution and `y'` is the predicted distribution. The goal during training is to minimize this loss.\n",
    "\n",
    "6. **Gradient Descent**: Gradient descent is an optimization algorithm used to minimize the loss function. It iteratively adjusts the parameters (weights and biases) in the direction that most decreases the cost function. The parameters are updated using the rule `θ = θ - η * ∇J(θ)`, where `θ` represents the parameters, `η` is the learning rate, and `∇J(θ)` is the gradient of the loss function.\n",
    "\n",
    "By iterating through these steps, softmax regression learns a model that can classify instances into one of multiple possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 1962.1213523984934\n",
      "Epoch 20: Loss = 2048.153984716841\n",
      "Epoch 30: Loss = 2059.505031700428\n",
      "Epoch 40: Loss = 2064.957678188899\n",
      "Epoch 50: Loss = 2068.417087881193\n",
      "Epoch 60: Loss = 2071.390218063033\n",
      "Epoch 70: Loss = 2072.2785484566075\n",
      "Epoch 80: Loss = 2072.3250234233046\n",
      "Epoch 90: Loss = 2072.3265341456513\n",
      "Epoch 100: Loss = 2072.3265821222817\n",
      "Final weights:\n",
      "[[  24.9121626    54.46180505]\n",
      " [  51.00844995   37.73787157]\n",
      " [  -2.332342   -113.93102528]]\n",
      "Final biases:\n",
      "[499.52812614 489.75059573]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(x):\n",
    "    # Subtracting the maximum value for numerical stability\n",
    "    exps = np.exp(x - np.max(x))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    # Avoiding numerical instability by adding a small epsilon\n",
    "    epsilon = 1e-9\n",
    "    return -np.sum(y_true * np.log(y_pred + epsilon))\n",
    "\n",
    "# Generate some sample data\n",
    "num_samples = 100\n",
    "num_features = 3\n",
    "num_classes = 2\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(num_samples, num_features)\n",
    "y = np.random.randint(num_classes, size=num_samples)\n",
    "\n",
    "# Initialize the weights and biases\n",
    "W = np.random.randn(num_features, num_classes)\n",
    "b = np.random.randn(num_classes)\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    logits = np.dot(X, W) + b\n",
    "    y_pred = softmax(logits)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = cross_entropy_loss(y_pred, np.eye(num_classes)[y])\n",
    "\n",
    "    # Backward pass\n",
    "    grad_logits = y_pred - np.eye(num_classes)[y]\n",
    "    grad_W = np.dot(X.T, grad_logits)\n",
    "    grad_b = np.sum(grad_logits, axis=0)\n",
    "\n",
    "    # Update the weights and biases\n",
    "    W -= learning_rate * grad_W\n",
    "    b -= learning_rate * grad_b\n",
    "\n",
    "    # Print the loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: Loss = {loss}\")\n",
    "\n",
    "# Print the final weights and biases\n",
    "print(\"Final weights:\")\n",
    "print(W)\n",
    "print(\"Final biases:\")\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sample(sample):\n",
    "    logits = np.dot(sample, W) + b\n",
    "    probabilities = softmax(logits)\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "# Define a new sample\n",
    "new_sample = np.array([1.2, 0.5, -0.8])\n",
    "\n",
    "# Use the predict_sample method with the new sample\n",
    "predicted_class = predict_sample(new_sample)\n",
    "\n",
    "# Print the predicted class\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
